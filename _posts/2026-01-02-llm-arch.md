---
title: "The Bottleneck Moved: Software Architecture After LLMs"
published: true
permalink: "/how-llms-affect-software-architecture"
share-img: /img/book.jpg
tags: [ai, llm, design, architecture, software]
readtime: true
comments: false
---


For years, we repeated the same comfortable sentence: “Architecture matters.” In the AI era, that sentence becomes less a belief and more a constraint. Not because LLMs are magical, but because they change the economics of building software in a way that exposes every weakness we used to tolerate.

Code is getting cheap—at least in the sense that producing *something that looks like code* is suddenly fast. With modern LLMs, agentic workflows, and tool-connection standards like MCP, the throughput of a single developer can jump dramatically. The bottleneck moves.

And when bottlenecks move, your architecture either becomes a force multiplier—or an amplifier of chaos.

The story many teams are living right now is familiar: you describe a feature, you get a feature. You describe an application, you get an application. Some people call it “vibe coding”: you steer with language, the model fills in implementation details, and momentum feels effortless—until it doesn’t. The uncomfortable part is not that the code is obviously bad. It’s that it often looks *extremely* legitimate. The style is fine. The structure is plausible. The names are well-chosen. The logic reads clean. And yet the behavior can be subtly wrong, dangerously incomplete, or misaligned with real constraints that were never made explicit.

In other words: AI doesn’t mainly produce garbage. It produces confidence.

That changes what “good engineering” means day to day.

Before LLMs, serious code review was disproportionately a senior responsibility—partly because experience helps you see failure modes quickly, and partly because review time was limited. Now, if your team can generate more code per week, review can no longer remain an elite activity. It becomes basic literacy. Not as a moral statement—simply because the volume of change grows while the cost of shipping a mistake stays the same (or increases).

This is also why testing quietly becomes the center of gravity. When a probabilistic engine is authoring a meaningful portion of your codebase, you need a deterministic counterpart: a harness that does not care whether the code “reads well,” only whether it behaves correctly. In AI-assisted development, tests stop being hygiene and start being the definition of reality. They are the anchor that turns high throughput into something you can trust.

And then there is the critical limitation that shapes everything: context.

LLMs do not reason over an infinite, stable world model of your system. They operate over a bounded, lossy slice of information. The larger and messier the context, the more likely they are to miss constraints, invent details, or “complete patterns” that feel right but aren’t real. This isn’t a moral failure of AI; it’s a mechanical fact of how the tool works.

If that’s true, then architecture stops being an abstract discipline about boxes and arrows. It becomes a practical strategy for keeping both humans and machines inside a tractable slice of reality.

## Why architecture becomes more important, not less

In the AI era, the value of architecture rises because architecture is what makes speed *safe*.

Small, isolated components—whether libraries, modules, or services—reduce the amount of information needed to make a correct change. They shrink the blast radius, narrow the review surface, and compress the “working set” that a developer (or an LLM) must hold in mind. The goal is not fragmentation for its own sake. The goal is to minimize the context required to do the next change correctly.

Clear interfaces become the next lever. When boundaries are explicit—OpenAPI for HTTP services, Protobuf/gRPC for RPC-style contracts, well-defined Kafka event schemas for async communication—you force the system to speak in machine-readable terms. That helps humans, but it also helps LLMs behave more like disciplined engineers: they can operate by consuming a contract rather than rummaging through internal implementation hoping to infer intent.

Once your boundaries are crisp and your contracts are real, something interesting happens: rewriting becomes a controlled experiment instead of a political event.

A team can take a component and replace it in days—or even hours—because the interface remains stable and the verification harness stays intact. You can test two implementations against the same E2E suite, benchmark them under load, and keep the better one. Before AI acceleration, “rewrite this service” often meant an undefined number of sprints and a long tail of integration pain. Now, the rewrite itself is less scary. The scary part is whether you can *prove* it’s equivalent (or better). That proof is architecture plus tests plus measurement.

This is where I think the winners will diverge sharply. Teams that treat AI as a code firehose will generate more output and accumulate more inconsistency. Teams that treat AI as a component-replacement engine—constrained by contracts, tests, and benchmarks—will accumulate leverage.

## Verification becomes the product

A subtle but fundamental shift follows from all of this: the highest-value artifacts are no longer the code itself, but the mechanisms that make the code safe to change.

As AI accelerates implementation, verification becomes the real differentiator.

End-to-end tests stop being optional because they catch the most common AI failure mode: code that is locally plausible but globally incompatible. Benchmarks stop being vanity metrics because performance regressions become easier to introduce when large volumes of code are rewritten quickly. Observability stops being a “nice to have,” because faster change demands tighter and faster feedback from production.

This is where architecture directly amplifies verification quality.

Smaller components and well-bounded services allow tests and benchmarks to become far more precise. Functional tests can target a single responsibility instead of navigating an entire system. Benchmarks can measure exactly what changed, instead of averaging noise across unrelated paths. When a component has a narrow purpose, failures are easier to attribute, regressions are easier to detect, and results are harder to misinterpret.

In large, entangled systems, verification becomes blunt and expensive. In small, isolated components, it becomes sharp and cheap.

If you want a crisp mental model: in the AI era, features are cheap; regressions are expensive; verification is the moat—and good architecture determines how defensible that moat actually is.

## Product evolution accelerates, roles do not collapse

There is a popular belief that in the AI era, software developers will turn into product managers—or that product managers will turn into developers—because building software is becoming trivial. I do not buy this.

Product development is not code assembly. It is market understanding, customer discovery, positioning, negotiation, and continuous feedback. Software engineering is not typing speed. It is system thinking, architecture, constraints, correctness, performance, and math. These are different disciplines. LLMs accelerate parts of both, but they do not erase the boundary between them—unless we assume the arrival of true AGI.

What *does* change is the distance between the roles.

AI compresses the gap between idea and execution. Engineers can prototype faster. Product hypotheses can be tested earlier. Variants can be shipped, measured, and discarded with far less friction. A/B tests, feature flags, and experimental flows stop being expensive decisions and become routine tools.

This only works if the system is designed for it.

Modular architecture makes product evolution safe. Small, well-bounded components allow teams to change behavior locally, test ideas in isolation, and roll back without collateral damage. Combined with AI-assisted implementation, this turns experimentation into a controlled process rather than a gamble.

The outcome is not role convergence, but tighter coupling. Teams where product and engineering operate closer—sharing context, iterating together, and grounding decisions in real system behavior—will move faster and learn faster. AI magnifies this advantage. Architecture decides whether that magnification produces insight or instability.

## A new reason strongly typed languages become more appealing

There is another shift I expect to grow in importance: strictly typed languages become more attractive—not primarily for ideology, but for physics.

When code is generated quickly, you want fast, automated ways to reject incorrect code before it even reaches review. A strong compiler is exactly that: a scalable gatekeeper that turns many classes of mistakes into immediate feedback. Even in languages that have long had compilers, the difference is how central compilation becomes to the AI workflow. The model can generate code, attempt to compile, read errors, and iterate—effectively using the compiler as a tool-assisted verifier. That feedback loop can be automated and brutally fast.

More importantly, rich type systems help humans keep track of AI-generated abstractions.

One risk of AI-assisted development is abstraction inflation: layers of helpers, wrappers, factories, and generic “clean architecture” shapes that look sophisticated but obscure intent. In a language with expressive types, you can force abstractions to carry meaning. You can model invariants in the type system, constrain illegal states, and encode contracts in a way that is difficult to “hand-wave” with pretty-looking code.

When the codebase is growing faster, types become a form of documentation that does not drift as easily, because the compiler enforces it. They also become a way to localize reasoning: you can understand what a function can and cannot do from its signature and constraints, rather than reading a page of generated implementation hoping it matches the story.

This doesn’t mean dynamic languages disappear. It means the comparative advantage of strong typing increases when the author is sometimes a machine and the pace of change is higher than your patience for ambiguity.

## Readme for humans, readme for AI

There is one more practical idea that becomes surprisingly effective in this world: treat each component as something that needs an onboarding packet not only for humans, but for AI-assisted work.

Many teams already maintain a README per service. In the AI era, it’s worth shaping that README so it can be pasted into an LLM context window and reliably constrain behavior. The most valuable content is not marketing text; it is operational truth: what the component does, what it must never do, the public contracts, the invariants, how to run tests, and what “good” looks like in performance and failure behavior.

If you do that, you’re not just helping the next developer. You are actively shrinking the context the model needs, which reduces hallucination and improves correctness.

## Summary

AI does not make architecture obsolete. It makes architecture measurable.

If LLMs speed up implementation, then clarity becomes more valuable, boundaries become more valuable, tests become more valuable, benchmarks become more valuable, and review discipline becomes more valuable. The teams that succeed will not be the teams that generate the most code. They will be the teams that turn generation into a controlled, testable, reversible process.

In that sense, architecture becomes your token budget: the structure that determines whether the next change fits inside a small, reliable slice of reality—or whether you’re about to ship confidence instead of correctness.
